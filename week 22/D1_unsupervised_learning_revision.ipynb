{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Unsupervised Learning](#toc1_)    \n",
        "  - [Dimensionality reduction](#toc1_1_)    \n",
        "    - [Principal Component Analysis](#toc1_1_1_)    \n",
        "    - [Extra: Autoencoders](#toc1_1_2_)    \n",
        "      - [**What are autoencoders good for?** $^{[2]}$ ](#toc1_1_2_1_)    \n",
        "      - [**So what's the big deal with autoencoders?** $^{[2]}$ ](#toc1_1_2_2_)    \n",
        "  - [Clustering](#toc1_2_)    \n",
        "    - [K-Means clustering](#toc1_2_1_)    \n",
        "    - [Agglomerative clustering](#toc1_2_2_)    \n",
        "      - [Ward linkage](#toc1_2_2_1_)    \n",
        "      - [Complete linkage](#toc1_2_2_2_)    \n",
        "      - [Single linkage](#toc1_2_2_3_)    \n",
        "    - [Clustering metrics](#toc1_2_3_)    \n",
        "    - [Number of clusters](#toc1_2_4_)    \n",
        "- [Resources](#toc2_)    \n",
        "- [References](#toc3_)    \n",
        "- [Acknowledgements](#toc4_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Unsupervised Learning](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwD_ZUaGN9b5"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z13Plu9XuzdG",
        "outputId": "dd438a8c-93f8-49dc-e883-4a8658c585a0"
      },
      "outputs": [],
      "source": [
        "# Unsupervised Learning Class\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()\n",
        "print(cancer['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[Dimensionality reduction](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_1_'></a>[Principal Component Analysis](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://imgs.search.brave.com/1u8pddu4FRUlSAuxj0i992lYN__bYby0JDUEqF2abEM/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9icmFk/bGV5Ym9laG1rZS5n/aXRodWIuaW8vSE9N/TC8xNS1wY2FfZmls/ZXMvZmlndXJlLWh0/bWwvY3JlYXRlLXBj/YS1pbWFnZS0xLnBu/Zw)  \n",
        "(Source: [Hands-on Machine Learning with R, Bradley Boehmke](https://bradleyboehmke.github.io/HOML/pca.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Tm0xE7cHJdRp",
        "outputId": "334c71f4-a493-4344-9592-2596d4537a5c"
      },
      "outputs": [],
      "source": [
        "X = cancer['data']\n",
        "y = cancer['target']\n",
        "display(X.shape)\n",
        "display(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "kV_rlc-RMdSo",
        "outputId": "95320427-f729-4de0-99ea-c216fb8396fb"
      },
      "outputs": [],
      "source": [
        "# let's visualize all the data in a dataframe\n",
        "data = pd.DataFrame(X, columns=cancer['feature_names'])\n",
        "data['label'] = y\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this dataset, we're not looking to predict whether or not a patient has a malignant/benign tumour, but if we can create 2 clusters corresponding to the most similar tumours. However, we think that the malignant tumours are significantly different from benign tumours, which is why we're looking to see if we can separate clusters based on the tumour type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "YRvszCv7M9jo",
        "outputId": "3959b5fe-3b82-4301-8a6b-4f180077f5bd"
      },
      "outputs": [],
      "source": [
        "#check differences between features and labels\n",
        "plot_options, charts = plt.subplots(5, 6, figsize=(20, 7))\n",
        "malignant = data[data['label'] == 1]\n",
        "benign = data[data['label'] == 0]\n",
        "#ravel flattens the array so we don't need 2 indexes\n",
        "charts_1d = charts.ravel()\n",
        "for i in range(30):\n",
        "  charts_1d[i].hist(malignant.iloc[:, i], bins=50, alpha=.5)\n",
        "  charts_1d[i].hist(benign.iloc[:, i], bins=50, alpha=.5)\n",
        "  charts_1d[i].set_title(data.columns[i])\n",
        "  plot_options.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ideally, we'd want to see if we can cluster the tumours based on multiple features (like we did for the iris dataset some time ago) but in this case we have >30 labels, which makes a pairplot take ages to run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9BHuscVtS3tq",
        "outputId": "e93ecc96-6049-49d4-8c95-176742a0dc72"
      },
      "outputs": [],
      "source": [
        "# sns.pairplot(data, hue = 'label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can still look to see if we can separate clusters based on 2 features at a time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "E3YO6CO32Rnu",
        "outputId": "d34cd53e-9e85-47e6-acb2-1a602a3a6d2d"
      },
      "outputs": [],
      "source": [
        "# Cluster boundary for features 1, 29\n",
        "fig = px.scatter(x=data.iloc[:, 1], y=data.iloc[:, 29], color=y, opacity=0.5)\n",
        "fig.update_layout(xaxis_title=data.columns[1], yaxis_title=data.columns[29], height=600, width=600)\n",
        "fig.show()\n",
        "\n",
        "# Cluster boundary for features 28, 29\n",
        "fig = px.scatter(x=data.iloc[:, 28], y=data.iloc[:, 29], color=y, opacity=0.5)\n",
        "fig.update_layout(xaxis_title=data.columns[28], yaxis_title=data.columns[29], height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmu3Xq34Tga7"
      },
      "outputs": [],
      "source": [
        "# Scale data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(cancer['data'])\n",
        "\n",
        "X_scaled = scaler.transform(cancer['data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why do we need to perform scaling before running the PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5ra54fGUW5t"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# create the PCA object\n",
        "# the number of components chosen will be the new number of features!\n",
        "pca = PCA(n_components=3)\n",
        "# fit the PCA model to breast cancer data\n",
        "pca.fit(X_scaled)\n",
        "# it's like we have three new axis (those defined by the PCA principal components)\n",
        "X_pca = pca.transform(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-dLdOMxe1wQ8",
        "outputId": "ca06f495-70a4-4169-86c3-5c40efea44de"
      },
      "outputs": [],
      "source": [
        "display(X.shape)\n",
        "display(X_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=y, opacity=0.5)\n",
        "fig.update_layout(xaxis_title='1st principal component', yaxis_title='2nd principal component', height=600, width=600)\n",
        "fig.show()\n",
        "\n",
        "fig = px.scatter(x=X_pca[:, 1], y=X_pca[:, 2], color=y, opacity=0.5)\n",
        "fig.update_layout(xaxis_title='2nd principal component', yaxis_title='3rd principal component', height=600, width=600)\n",
        "fig.show()\n",
        "\n",
        "fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 2], color=y, opacity=0.5)\n",
        "fig.update_layout(xaxis_title='1st principal component', yaxis_title='3rd principal component', height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to choose the number of principal components?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-Y-iS3dU9ST",
        "outputId": "a59a8aac-1280-4a2f-db0f-a96cde367b92"
      },
      "outputs": [],
      "source": [
        "# Each PCA feature has a little bit of each original feture\n",
        "# The PCA tells you how much of the original features each new feature contains\n",
        "pca.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matplotlib\n",
        "plt.matshow(pca.components_, cmap='viridis')\n",
        "plt.yticks([0, 1, 2], [\"First component\", \"Second component\",\"Third component\"])\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(data.columns)), data.columns, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Plotly\n",
        "# fig = px.imshow(pca.components_, y=['1st component', '2nd component', '3rd component'], x=list(data.columns.drop('label')), color_continuous_scale='viridis')\n",
        "# fig.update_coloraxes(overwrite=True)\n",
        "# fig.update_layout(showlegend=False)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Mathematically, the new features (principal components) are a LINEAR COMBINATION of the previous (old) features and the weights of each of them are represented in the diagram above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_2_'></a>[Extra: Autoencoders](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://imgs.search.brave.com/K3g5D5VJuFuoNO4ruHvGxPORZkLhj_n8SKQIasDrgcE/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9wbHVy/YWxzaWdodDIuaW1n/aXgubmV0L2d1aWRl/cy9kMDJlN2YwMC0y/YWYyLTRjZGYtYmNj/YS0xNWNlNGVhZjQ5/MzVfMTYuSlBH)  \n",
        "(Source: [PluralSight](https://www.pluralsight.com))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Autoencoders are the fancier version of PCA, which use neural networks in order to do dimensionality reduction and allow for non-linear combinations of the features. They can be very useful in image compression.\n",
        "\n",
        "> PCA vs Autoencoder[$^{[1]}$](https://www.geeksforgeeks.org/how-is-autoencoder-different-from-pca/)\n",
        "> - Although PCA is fundamentally a linear transformation, auto-encoders may describe complicated non-linear processes.\n",
        "> - Because PCA features are projections onto the orthogonal basis, they are completely linearly uncorrelated. However, since autoencoded features are only trained for correct reconstruction, they may have correlations.\n",
        "> - PCA is quicker and less expensive to compute than autoencoders.\n",
        "> - PCA is quite similar to a single layered autoencoder with a linear activation function.\n",
        "> - Because of the large number of parameters, the autoencoder is prone to overfitting. (However, regularization and proper planning might help to prevent this)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_1_2_1_'></a>**What are autoencoders good for?** [$^{[2]}$](https://blog.keras.io/building-autoencoders-in-keras.html)  [&#8593;](#toc0_)\n",
        "> They are **rarely used in practical applications**. In 2012 they briefly found an application in greedy layer-wise pretraining for deep convolutional neural networks, but this quickly fell out of fashion as we started realizing that better random weight initialization schemes were sufficient for training deep networks from scratch. In 2014, batch normalization started allowing for even deeper networks, and from late 2015 we could train arbitrarily deep networks from scratch using residual learning.\n",
        "\n",
        "> Today two interesting practical applications of autoencoders are **data denoising** (which we feature later in this post), and **dimensionality reduction for data visualization**. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_1_2_2_'></a>**So what's the big deal with autoencoders?** [$^{[2]}$](https://blog.keras.io/building-autoencoders-in-keras.html)  [&#8593;](#toc0_)\n",
        "> Their main claim to fame comes from **being featured in many introductory machine learning classes available online**. As a result, a lot of newcomers to the field absolutely love autoencoders and can't get enough of them. This is the reason why this tutorial exists!\n",
        "\n",
        "> Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a **self-supervised technique**, a specific instance of supervised learning where **the targets are generated from the input data**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[Clustering](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_1_'></a>[K-Means clustering](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://cdn.sanity.io/images/kuana2sp/production/4a7a2b92082d482c56e0c6396064ca23074168ac-1020x752.png?w=1080&fit=max&auto=format)  \n",
        "(Source: [Getting started with k-means clustering in Python, Dr J Rogel-Salazar](https://domino.ai/blog/getting-started-with-k-means-clustering-in-python))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "p6nIROyG6n9L",
        "outputId": "0728c9a8-2684-404f-941e-824232c867a5"
      },
      "outputs": [],
      "source": [
        "# K-means\n",
        "from sklearn import cluster, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# Creating 3 clusters artificially to illustrate the algorithm\n",
        "n_samples = 1500\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, centers=3, cluster_std=0.7, n_features=2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Review initial data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1],alpha=0.5,)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bblvBze7q1Gm"
      },
      "outputs": [],
      "source": [
        "# Matplotlib\n",
        "# plt.style.use('classic')\n",
        "\n",
        "# Plotly\n",
        "import plotly.io as pio\n",
        "pio.templates.default = 'simple_white'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run clustering with:\n",
        "\n",
        "a) k = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "K9Dq6mveD5NA",
        "outputId": "cf438724-f850-480e-8ff7-184faadcaf22"
      },
      "outputs": [],
      "source": [
        "kmeans = cluster.KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "pred = kmeans.predict(X)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred, alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) k = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "StVvxxOhDghk",
        "outputId": "dbcbe303-68fc-484a-fcc1-3e48a19e0926"
      },
      "outputs": [],
      "source": [
        "kmeans = cluster.KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "pred = kmeans.predict(X)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred,alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c) k = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "1s6LnkHE6n_9",
        "outputId": "5beb7a03-e208-458f-91e5-78eecf507d8f"
      },
      "outputs": [],
      "source": [
        "kmeans = cluster.KMeans(n_clusters=4)\n",
        "kmeans.fit(X)\n",
        "pred = kmeans.predict(X)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred,alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_2_'></a>[Agglomerative clustering](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20200204181551/Untitled-Diagram71.png)  \n",
        "(Source: [Hierarchical Clustering in Machine Learning, Geeks 4 Geeks](https://www.geeksforgeeks.org/ml-hierarchical-clustering-agglomerative-and-divisive-clustering/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw_p6wjj6oC-"
      },
      "outputs": [],
      "source": [
        "from sklearn import cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_2_2_1_'></a>[Ward linkage](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "d0MkbX9PRHKj",
        "outputId": "f7f0aeaa-8d95-472a-92a9-77a219c27779"
      },
      "outputs": [],
      "source": [
        "# ward linkage tends to produce relatively equally sized clusters\n",
        "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "pred = agglomerative.fit_predict(X)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred, alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_2_2_2_'></a>[Complete linkage](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "QznfD0ONRmC6",
        "outputId": "5189539b-ec7d-4f42-e059-4a45ad94be1c"
      },
      "outputs": [],
      "source": [
        "# complete linkage penalizes heavily outliers\n",
        "agglomerative = cluster.AgglomerativeClustering(n_clusters=3,linkage='complete')\n",
        "pred = agglomerative.fit_predict(X)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred,alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_2_2_3_'></a>[Single linkage](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "l2X6SSTfSWEU",
        "outputId": "011b97e2-ee73-4361-dee0-48e2cc7da666"
      },
      "outputs": [],
      "source": [
        "# different algorithms are good for different applications\n",
        "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='single')\n",
        "pred = agglomerative.fit_predict(X)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred,alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "zzrjXjRGl-rj",
        "outputId": "34d61f96-e3e7-46a0-b5b0-e87f6939aefc"
      },
      "outputs": [],
      "source": [
        "# different algorithms are good for different applications - kmeans and single agglomerative have so far shown very different results\n",
        "\n",
        "# Create different type of clusters\n",
        "n_samples = 1500\n",
        "X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Run K-Means on non-radial clusters\n",
        "kmeans = cluster.KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y1_pred = kmeans.predict(X)\n",
        "\n",
        "# Run Agglomerative clustering on non-radial clusters\n",
        "single = cluster.AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y2_pred = single.fit_predict(X)\n",
        "\n",
        "# Review results\n",
        "options, charts = plt.subplots(1, 2, figsize=(12, 6))\n",
        "colors = np.array(['blue', 'red'])\n",
        "charts[0].scatter(X[:, 0], X[:, 1], color=colors[y1_pred])\n",
        "charts[1].scatter(X[:, 0], X[:, 1], color=colors[y2_pred])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_3_'></a>[Clustering metrics](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Silhouette Score** [$^{[3]}$](https://www.educative.io/answers/what-is-silhouette-score)\n",
        "\n",
        "> Silhouette Score is a tool for assessing the appropriateness of clustering results by providing a quantitative measure of how well-defined and distinct the clusters are. The Silhouette Score quantifies **how well a data point fits into its assigned cluster** and **how distinct it is from other clusters** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Silhouette Score = $\\frac{(b - a)}{max(a, b)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "LKLMgR4bgpMy",
        "outputId": "3701dc2a-aef9-49a0-fe88-c4091d2ea4e3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Re-create clusters\n",
        "n_samples = 1500\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, centers=3, cluster_std=0.7, n_features=2, random_state=0)\n",
        "\n",
        "kmeans = cluster.KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "pred = kmeans.predict(X)\n",
        "\n",
        "print(\"Model 1 Silhouette Score: {}\".format(silhouette_score(X, pred, metric='euclidean')))\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred, alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "AXAoqsPjhHhC",
        "outputId": "a84a1c94-191c-4489-c6b6-32a18308900d"
      },
      "outputs": [],
      "source": [
        "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "pred = agglomerative.fit_predict(X)\n",
        "\n",
        "print(\"Model 2 Silhouette Score: {}\".format(silhouette_score(X, pred, metric='euclidean')))\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred, alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "TmebPqlAhM83",
        "outputId": "4abd74ec-8e2b-409e-f670-be412fc059b2"
      },
      "outputs": [],
      "source": [
        "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='single')\n",
        "pred = agglomerative.fit_predict(X)\n",
        "\n",
        "print(\"Model 3 Silhouette Score: {}\".format(silhouette_score(X, pred, metric='euclidean')))\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=pred, alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The Silhouette Score ranges from -1 to +1. Here is how to interpret the value: [$^{[3]}$](https://www.educative.io/answers/what-is-silhouette-score)\n",
        "\n",
        "**Negative**\n",
        "> A negative score indicates that the **data point is likely assigned to the wrong cluster**, as its distance to its assigned cluster’s points is greater than its distance to the nearest neighboring cluster’s points.\n",
        "\n",
        "**Close to 0**\n",
        "> A score close to 0 implies that the **data point is on or very close to the decision boundary between two clusters**. It indicates that the clustering is not well-defined and can be ambiguous.\n",
        "\n",
        "**Positive**\n",
        "> A positive score indicates that the **data point is appropriately clustered**, and **its distance to its assigned cluster’s points is smaller than its distance to the nearest neighboring cluster’s points**. A score close to +1 suggests that the data point is well-clustered and distinctly separated from other clusters. It is a strong indication of a meaningful clustering result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where does the silhouette score fail to be a good measure?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 1500\n",
        "X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "kmeans = cluster.KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y1_pred = kmeans.predict(X)\n",
        "\n",
        "\n",
        "single = cluster.AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y2_pred = single.fit_predict(X)\n",
        "\n",
        "\n",
        "options, charts = plt.subplots(1, 2, figsize=(12, 6))\n",
        "colors = np.array(['blue', 'red'])\n",
        "charts[0].scatter(X[:, 0], X[:, 1], color=colors[y1_pred])\n",
        "charts[1].scatter(X[:, 0], X[:, 1], color=colors[y2_pred])\n",
        "plt.show()\n",
        "\n",
        "print(\"Model 1 Silhouette Score: {}\".format(silhouette_score(X, y1_pred)))\n",
        "print(\"Model 2 Silhouette Score: {}\".format(silhouette_score(X, y2_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_4_'></a>[Number of clusters](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "APku72twna_q",
        "outputId": "145cb02d-c6b7-4c41-e33c-7c9960b9818e"
      },
      "outputs": [],
      "source": [
        "from sklearn import cluster, datasets\n",
        "\n",
        "n_samples = 1500\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, centers=12, cluster_std=1, n_features=2, random_state=1)\n",
        "\n",
        "# Matplotlib\n",
        "# plt.figure(figsize=(5, 5))\n",
        "# plt.scatter(X[:, 0], X[:, 1],alpha=0.5)\n",
        "# plt.show()\n",
        "\n",
        "# Plotly\n",
        "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
        "fig.update_layout(height=600, width=600)\n",
        "fig.show()\n",
        "\n",
        "# even though data was generated with 12 centers, 5 clusters could make sense, even 7 or 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can select a number of clusters using the elbow heuristic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "h2pkUPRFhjZj",
        "outputId": "00192ff6-62fd-4fac-e586-da3273e9191d"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "model = cluster.KMeans()\n",
        "visualizer = KElbowVisualizer(model, k=(2,20))\n",
        "visualizer.fit(X)\n",
        "visualizer.poof()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Technically, any k between 4 and 8 would be good enough for this problem. Remember, we're not looking to minimize the error in this case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "UNNZH_s9nOTf",
        "outputId": "6dc7e0fa-e858-467f-fbc8-77c9204d7750"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "model = cluster.KMeans()\n",
        "visualizer = KElbowVisualizer(model, k=(1495,1499))\n",
        "visualizer.fit(X)\n",
        "visualizer.poof()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we try to minimize the error, we end up with an unusable number of clusters, which defeats the purpose of running the algorithm to begin with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Resources](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PCA by StatQuest:\n",
        "- [Main ideas (5 min)](https://www.youtube.com/watch?v=HMOI_lkzW08)\n",
        "- [Step-by-step (22 min)](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
        "- [Practical Tips (8 min)](https://www.youtube.com/watch?v=oRvgq966yZg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[References](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[1] [How is Autoencoder different from PCA, Geeks 4 Geeks](https://www.geeksforgeeks.org/how-is-autoencoder-different-from-pca/)  \n",
        "[2] [Building Autoencoders in Keras, Keras Blog](https://blog.keras.io/building-autoencoders-in-keras.html)  \n",
        "[3] [What is Silhouette Score, Educative IO](https://www.educative.io/answers/what-is-silhouette-score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc4_'></a>[Acknowledgements](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thank you, David Henriques, for your awesome lesson structure and content!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
