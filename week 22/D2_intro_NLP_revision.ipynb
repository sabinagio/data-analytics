{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Structured vs Unstructured Data](#toc1_)    \n",
        "  - [Structured Data](#toc1_1_)    \n",
        "    - [Pros $^{[1]}$      ](#toc1_1_1_)    \n",
        "    - [Cons $^{[1]}$      ](#toc1_1_2_)    \n",
        "    - [Tools](#toc1_1_3_)    \n",
        "  - [Unstructured Data](#toc1_2_)    \n",
        "    - [Pros $^{[1]}$      ](#toc1_2_1_)    \n",
        "    - [Cons $^{[1]}$      ](#toc1_2_2_)    \n",
        "    - [Tools](#toc1_2_3_)    \n",
        "- [Intro to NLP](#toc2_)    \n",
        "  - [Text preprocessing](#toc2_1_)    \n",
        "    - [Tokenization](#toc2_1_1_)    \n",
        "      - [Word tokenization](#toc2_1_1_1_)    \n",
        "      - [Sentence tokenization](#toc2_1_1_2_)    \n",
        "    - [POS tagging](#toc2_1_2_)    \n",
        "    - [Stemming](#toc2_1_3_)    \n",
        "    - [Lemmatization](#toc2_1_4_)    \n",
        "    - [Stopwords removal](#toc2_1_5_)    \n",
        "    - [Vectorization - Bag of Words (BoW) model](#toc2_1_6_)    \n",
        "  - [News clustering](#toc2_2_)    \n",
        "    - [Extract news data](#toc2_2_1_)    \n",
        "    - [Preprocess text](#toc2_2_2_)    \n",
        "    - [Vectorization](#toc2_2_3_)    \n",
        "    - [Clustering](#toc2_2_4_)    \n",
        "- [Resources](#toc3_)    \n",
        "- [References](#toc4_)    \n",
        "- [Acknowledgements](#toc5_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Structured vs Unstructured Data](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://imgs.search.brave.com/-3045xn2hR2tPAVWfaseQTUx2r88o-SkOSWMKpPrRNc/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9sYXd0/b21hdGVkLmNvbS93/cC1jb250ZW50L3Vw/bG9hZHMvMjAxOS8w/NC9zdHJ1Y3R1cmVk/VnNVbnN0cnVjdHVy/ZWRJZ25lb3MucG5n)  \n",
        "(Source: [Structured vs Unstructured Data: An Overview, Mongo DB](https://www.mongodb.com/unstructured-data/structured-vs-unstructured))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[Structured Data](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://images.surferseo.art/970da28a-3eeb-45ec-a3bf-1f9dce66c94d.png)  \n",
        "(Source: PhoenixNAP Global IT Services, June 2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_1_'></a>Pros [$^{[1]}$](https://www.mongodb.com/unstructured-data/structured-vs-unstructured)       [&#8593;](#toc0_)\n",
        "> - **Easily used by machine learning (ML) algorithms**: The specific and organized architecture of structured data eases manipulation and querying of ML data.\n",
        "> - **Easily used by business users**: Structured data does not require an in-depth understanding of different types of data and how they function. With a basic understanding of the topic relative to the data, users can easily access and interpret the data.\n",
        "> - **Accessible by more tools**: Since structured data predates unstructured data, there are more tools available for using and analyzing structured data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_2_'></a>Cons [$^{[1]}$](https://www.mongodb.com/unstructured-data/structured-vs-unstructured)       [&#8593;](#toc0_)\n",
        "> - **Limited usage**: Data with a predefined structure can only be used for its intended purpose, which limits its flexibility and usability.\n",
        "> - **Limited storage options**: Structured data is generally stored in data storage systems with rigid schemas (e.g., “data warehouses”). Therefore, changes in data requirements necessitate an update of all structured data, which leads to a massive expenditure of time and resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_3_'></a>[Tools](#toc0_)\n",
        "\n",
        "- mainly SQL-based databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[Unstructured Data](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://www.egnyte.com/sites/default/files/inline-images/zITEmudg0OvGfRblGApjWuFu20xY1NCNAnmu8O52KKtD4FLSPG.png)  \n",
        "(Source: [What Is Unstructured Data?, Egnyte](https://www.egnyte.com/guides/governance/unstructured-data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_1_'></a>Pros [$^{[1]}$](https://www.mongodb.com/unstructured-data/structured-vs-unstructured)       [&#8593;](#toc0_)\n",
        "> - **Native format**: Unstructured data, stored in its native format, remains undefined until needed. Its adaptability increases file formats in the database, which widens the data and enables data scientists to prepare and analyze only the data they need.\n",
        "> - **Fast accumulation rates**: Since there is no need to predefine the data, it can be collected quickly and easily.\n",
        "> - **Data lake storage**: Allows for massive storage and pay-as-you-use pricing, which cuts costs and eases scalability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_2_'></a>Cons [$^{[1]}$](https://www.mongodb.com/unstructured-data/structured-vs-unstructured)       [&#8593;](#toc0_)\n",
        "> - **Requires expertise**: Due to its undefined/non-formatted nature, data science expertise is required to prepare and analyze unstructured data. This is beneficial to data analysts but alienates unspecialized business users who may not fully understand specialized data topics or how to utilize their data.\n",
        "> - **Specialized tools**: Specialized tools are required to manipulate unstructured data, which limits product choices for data managers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_3_'></a>[Tools](#toc0_)\n",
        "\n",
        "- MongoDB, DynamoDB (AWS), Hadoop, Azure (Microsoft), S3 (AWS), also known as No-SQL databases :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Intro to NLP](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You know the drill\n",
        "# !pip install nltk\n",
        "# !pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(nltk.__version__)    # I have 3.8.1\n",
        "print(spacy.__version__)   # I have 3.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZl6x21RYTwt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy #if this doesn't work after installation, you might need to create another environment\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"The US State Department has just issued a statement on the meeting between Blinken and Abbas earlier today in Ramallah.\n",
        "\n",
        "“Secretary Blinken discussed ongoing efforts to minimize civilian harm in Gaza and accelerate and increase the delivery of humanitarian assistance to Palestinian civilians throughout Gaza,” the statement quoted spokesperson Matthew Miller as saying.\n",
        "\n",
        "Blinken also “noted increased volatility” in the occupied West Bank and discussed US efforts to address “extremist violence”.\n",
        "\n",
        "“He also underscored the United States’ position that all Palestinian tax revenues collected by Israel should be consistently conveyed to the Palestinian Authority in accordance with prior agreements,” Miller said, adding that the US “supports tangible steps towards the creation of a Palestinian state alongside the State of Israel, with both living in peace and security”.\n",
        "\n",
        "Earlier, we reported that during the meeting Abbas discussed the efforts made to stop the Israeli war on Gaza and the importance of accelerating the entry of aid into the bombarded territory.\n",
        "\n",
        "For its part, Hamas denounced Blinken’s visit to the region saying the US official’s “attempts to justify the genocide committed by the Israeli occupation army against Palestinian civilians … are miserable attempts to wash the hands of the criminal occupation of the blood of children, women and the elderly of Gaza”.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDzmoXCosE-6"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Satya Nadella, the chief executive of Microsoft, OpenAI’s biggest investor with a 49% stake, led mediation efforts that were complicated by Altman’s reported insistence that OpenAI’s board be removed as a precondition for his return. The interim CEO, Mira Murati, OpenAI’s chief technology officer, signalled her support for Altman’s return by posting a heart emoji next to her former colleague’s post professing love for OpenAI.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_1_'></a>[Text preprocessing](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_1_'></a>[Tokenization](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Tokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze. [$^{[2]}$](https://www.datacamp.com/blog/what-is-tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of tokenization: [$^{[2]}$](https://www.datacamp.com/blog/what-is-tokenization)\n",
        "\n",
        "- **Multiple word tokenization.** Breaking down text into groups of words (n-grams) or sentences. This can be relevant for elements that typically go together such as \"New York\", or for analyzing larger corpora of data, e.g. books.\n",
        "> - **Word tokenization**. Breaking text down into individual words. It's the most common approach and is particularly effective for languages with clear word boundaries like English. All NLP packages support this.\n",
        ">\n",
        "> - **Character tokenization**. Breaking text down into individual characters. This method is beneficial for languages that lack clear word boundaries (e.g. Chinese, Japanese, Arabic) or for tasks that require a granular analysis, such as spelling correction. For Chinese and Arabic, you can use the [Stanford Word Segmenter](https://nlp.stanford.edu/software/segmenter.shtml) from the NLTK library.\n",
        ">         \n",
        "> - **Subword tokenization**. Striking a balance between word and character tokenization, this method breaks text into units that might be larger than a single character but smaller than a full word. For instance, \"Chatbots\" could be tokenized into \"Chat\" and \"bots\". This approach is especially useful for languages that form meaning by combining smaller units (e.g. German) or when dealing with out-of-vocabulary words in NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc2_1_1_1_'></a>[Word tokenization](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLjXsXqzacqc",
        "outputId": "f26a7c26-282f-4f36-9f59-babb81ec16e8"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tokens[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We often remove punctuation after tokenization since punctuation is unlikely to be a good predictive feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ebc1CTzigA",
        "outputId": "dfc6d726-0e50-4148-d7f4-bad4a35a7458"
      },
      "outputs": [],
      "source": [
        "tokens = [word for word in tokens if word.isalnum()]\n",
        "tokens[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc2_1_1_2_'></a>[Sentence tokenization](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci7FNYAGbZV7",
        "outputId": "d3523cb5-8c4f-4d9c-a0fe-80a42fecafe2"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_2_'></a>[POS tagging](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Part of speech can be a useful feature in itself, but is also heavily used in making lemmatization and stemming more effective:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AMgqkEesfqr",
        "outputId": "6fb66ec2-9a4a-404a-bf36-f0bb767a1488"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(tokens, lang='eng')[:15]\n",
        "#explanation of all these codes can be found here: https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Spacy does the tokenization under the hood\n",
        "for token in doc:\n",
        "    if (not token.is_punct) and (not token.is_space): # but it doesn't automatically remove stopwords and punctuation\n",
        "        print(token, token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spacy also gives the option to visualize the POS tagging\n",
        "from spacy import displacy\n",
        "displacy.serve(doc, style=\"dep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "POS tagging can be very useful in disambiguating words that have different meanings depending on context. For example, you could distinguish between Thermos (the brand) and thermos (the commonly used item) and also identify what other words it is connected to. Additionally, it can also be used in NER (Named Entity Recognition), machine translation, and other downstream tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_3_'></a>[Stemming](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stemming is an easy method to decrease the size of our vocabulary by shortening words (i.e. removing suffixes/prefixes) to the minimum viable length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpq-YUJ7fc6t",
        "outputId": "323fb25c-4b08-4ff3-ae15-69e9658d2f48"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed = [ps.stem(w) for w in tokens]\n",
        "stemmed[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, stemming doesn't care about the semantics (meaning) of those words, which can result in words with different meaning being treated as the same word. That's when we would prefer to use lemmatization instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_4_'></a>[Lemmatization](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Lemmatization is a more context-aware version of stemming, where we take the actual roots of individual words. However, lemmatization needs an understanding of the language to work, meaning that for under-represented languages (e.g. African languages), stemming may be more appropriate, if the language allows it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEgJ4I8Hgo28",
        "outputId": "bfa56546-77b9-4346-b7ba-6c7b5ca9fecf"
      },
      "outputs": [],
      "source": [
        "# Wordnet is the most well known lemmatizer for english\n",
        "nltk.download('wordnet') \n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "lemmatized[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Lemmatization may still be a bit weak (e.g. `issued` should ideally be reduced to `issue`), mostly because the lemmatizer would like a bit more information about context to make decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "jHC45jlvuyN6",
        "outputId": "35931ad2-5f12-4511-8644-e0fa98598710"
      },
      "outputs": [],
      "source": [
        "display(lemmatizer.lemmatize(\"was\"))\n",
        "# display(lemmatizer.lemmatize(\"was\", wordnet.VERB))\n",
        "display(lemmatizer.lemmatize(\"better\"))\n",
        "# display(lemmatizer.lemmatize(\"better\", wordnet.ADJ))\n",
        "display(lemmatizer.lemmatize(\"canning\"))\n",
        "# display(lemmatizer.lemmatize(\"canning\", wordnet.NOUN))\n",
        "# display(lemmatizer.lemmatize(\"canning\", wordnet.VERB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given there's a mismatch between the Wordnet POS tagging and the previous POS tagging, we need to map them before using them to inform our lemmatizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P6i0MpYrawb",
        "outputId": "fb1d2026-0963-473c-9fc9-43b7d1d5070f"
      },
      "outputs": [],
      "source": [
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# unfortunately pos_tag and lemmatize use different codes for parts of speech\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper() # gets first letter of POS categorization\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) # get returns second argument if first key does not exist\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in tokens]\n",
        "lemmatized[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's see how spaCy approaches it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spacy_lemma = [token.lemma_ for token in doc if (not token.is_punct and not token.is_space)]\n",
        "spacy_lemma[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_5_'></a>[Stopwords removal](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stopwords are words that support the structure of a language without providing additional meaning to it. Thus, removal of stopwords allows us to reduce the noise in the data and focus on the words that carry meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPRDgJFo0cCw",
        "outputId": "742423b9-ce76-4b53-a155-f6209bf113a6"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "without_sw = [word for word in lemmatized if not word in stopwords.words()]\n",
        "without_sw[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "BCBqKEUP36wf",
        "outputId": "3794bf48-885b-4e26-eb21-550d62cfe146"
      },
      "outputs": [],
      "source": [
        "# Let's check the full sentence\n",
        "\" \".join(without_sw[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also try with spaCy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Uncomment to download the package of small models for English\n",
        "# !python -m spacy download en_core_web_sm  \n",
        "# To find a list of languages available, you can check out https://spacy.io/usage/models#languagess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "words_spacy = []\n",
        "for token in doc:\n",
        "    if (not token.is_stop) and (not token.is_punct) and (not token.is_space):\n",
        "        words_spacy.append(token.lemma_)\n",
        "words_spacy[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's check the full sentence\n",
        "\" \".join(str(word) for word in words_spacy[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, while spacy removes \"the\" from the token list, it also removes US (country) from the list, as it interprets it as us (pronoun)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_6_'></a>[Vectorization - Bag of Words (BoW) model](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://www.mathworks.com/discovery/bag-of-words/_jcr_content/mainParsys/columns/8977d091-c0d0-4e65-925d-2d3cd856939c/image.adapt.full.medium.jpg/1686734791097.jpg)  \n",
        "(Source: [What Is a Bag-of-Words?, Mathworks](https://www.mathworks.com/discovery/bag-of-words.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Oqk5EIM915rt",
        "outputId": "01f6eff2-be1f-4fb6-8fc5-4dd22a89f218"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vect = CountVectorizer()\n",
        "# Fit creates one entry for each different word seen\n",
        "bow_vect.fit([\" \".join(without_sw)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "without_sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWcjuCLI1ny4",
        "outputId": "d72e3a27-300b-4d53-9c6c-a426c64bbb33"
      },
      "outputs": [],
      "source": [
        "bow_vect.transform([text]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2s4dykkqL19",
        "outputId": "82b8881b-e466-4705-860c-ba460171866e"
      },
      "outputs": [],
      "source": [
        "bow_vect.transform(['gaza civilian today gaza']).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform only considers the words it has seen during the training (fitting) stage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNkASYyV2DH3"
      },
      "outputs": [],
      "source": [
        "bow_vect.transform(['gaza strip isi']).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bag of Words model is a very simple model to transform unstructured text data to structured tabular data. However, it has a few shortcomings:\n",
        "- It fails to recognize the meaning of word combinations, e.g. not bad, don't do. This can be solved by adding also combinations of 2 (2-grams) or more words (n-grams) in the vectorizing method.\n",
        "- It doesn't take into account context, which means it may fail to pick up on things like sarcasm. This is usually solved by employing word embedding models, which we'll discuss in the next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_2_'></a>[News clustering](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_1_'></a>[Extract news data](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Corpus of 120k news headlines, here shortened to 10k:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YgOIJ55E15xv",
        "outputId": "e354679c-8a66-40e8-c6f3-e3781a4452e8"
      },
      "outputs": [],
      "source": [
        "all_news = pd.read_csv('https://raw.githubusercontent.com/sabinagio/data-analytics/main/data/news.csv')\n",
        "print(all_news.shape)\n",
        "all_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "A1yquYlKtKt-",
        "outputId": "cf45c49f-eece-4211-8ba0-2aaa491397c1"
      },
      "outputs": [],
      "source": [
        "all_news.iloc[100]['news']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_2_'></a>[Preprocess text](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do the same process as before, except now we do it for all news pieces:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XCzORQ3jEvcE",
        "outputId": "a98af040-67ab-4dc5-b862-b87e1793e222"
      },
      "outputs": [],
      "source": [
        "# Tokenization, lowercasing, removing punctuation\n",
        "def tokenizer_and_remove_punctuation(row):\n",
        "  tokens = word_tokenize(row['news'])\n",
        "  return [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "all_news['tokenized'] = all_news.apply(tokenizer_and_remove_punctuation, axis=1)\n",
        "all_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HnvmW_m_Eve8",
        "outputId": "7637232a-5d0b-4fdb-f120-e906668bc248"
      },
      "outputs": [],
      "source": [
        "# Lemmatization using POS tags\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer_with_pos(row):\n",
        "  return [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in row['tokenized']]\n",
        "\n",
        "all_news['lemmatized'] = all_news.apply(lemmatizer_with_pos, axis=1) # This one will take a while\n",
        "all_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vViv80x-GsAT",
        "outputId": "25f1dbc2-846b-45cb-a81a-5a85e0a5aab5"
      },
      "outputs": [],
      "source": [
        "# Stopwords removal\n",
        "def remove_sw(row):\n",
        "  return list(set(row['lemmatized']).difference(stopwords.words()))\n",
        "\n",
        "all_news['no_stopwords'] = all_news.apply(remove_sw, axis=1)\n",
        "all_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "CX2m2ZQQLYqo",
        "outputId": "0dacab20-bdd4-4632-de52-5bca5cfa3933"
      },
      "outputs": [],
      "source": [
        "# Re-creating the sentences with the preprocessed text\n",
        "def recreate_sentences(row):\n",
        "  return \" \".join(row['no_stopwords'])\n",
        "\n",
        "all_news['clean_text'] = all_news.apply(recreate_sentences, axis=1)\n",
        "all_news.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_3_'></a>[Vectorization](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ4qJpciFrsS"
      },
      "outputs": [],
      "source": [
        "# We will use only the most common 1000 words\n",
        "bow_vect = CountVectorizer(max_features=1000)\n",
        "# Fit transform creates one entry for each different word seen\n",
        "X = bow_vect.fit_transform(all_news['clean_text']).toarray()\n",
        "as_df = pd.DataFrame(X, columns=bow_vect.get_feature_names_out())\n",
        "as_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_4_'></a>[Clustering](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY1KjIAuFr2V",
        "outputId": "c8ef51c7-5c77-450f-a055-67eba43dff56"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=10, random_state=100)\n",
        "kmeans.fit(X)\n",
        "pred = kmeans.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJwga-gM7-UA",
        "outputId": "8efdfc0a-6ea2-4532-f3fe-ab6ddc807dc9"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vV4UfDknFr6s",
        "outputId": "b17acff3-cece-4bb7-fcaa-38faa2e01522"
      },
      "outputs": [],
      "source": [
        "predict_df = pd.concat([all_news['news'], pd.DataFrame(pred, columns=['class'])], axis=1)\n",
        "predict_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check out the clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "_ACgfNccRG2W",
        "outputId": "8462948b-8ef1-4967-c27d-cf1c6e91a581"
      },
      "outputs": [],
      "source": [
        "predict_df[predict_df['class']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Ag0fW6pWQ28G",
        "outputId": "b31d7532-ba1c-42da-febc-6ff4f58a2586"
      },
      "outputs": [],
      "source": [
        "predict_df[predict_df['class']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "N_HN_-vNPLye",
        "outputId": "427d0ec0-9adb-4986-cc4e-3161c0db085e"
      },
      "outputs": [],
      "source": [
        "predict_df[predict_df['class']==4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "SplXmrDDxL2s",
        "outputId": "58a1c766-3d30-4b5b-a476-65c3a48ed767"
      },
      "outputs": [],
      "source": [
        "predict_df[predict_df['class']==5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[Resources](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- spaCy tutorial for beginners, made by spaCy creators: [Advanced NLP with spaCy](https://spacy.io/universe/project/spacy-course)\n",
        "- NLP Intro (full of maths): [NLP 4 You](https://lena-voita.github.io/nlp_course.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc4_'></a>[References](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[1] [Structured vs Unstructured Data: An Overview, Mongo DB](https://www.mongodb.com/unstructured-data/structured-vs-unstructured)  \n",
        "[2] [What is tokenization?, DataCamp](https://www.datacamp.com/blog/what-is-tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc5_'></a>[Acknowledgements](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thank you, David Henriques, for your awesome lesson structure & content!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
