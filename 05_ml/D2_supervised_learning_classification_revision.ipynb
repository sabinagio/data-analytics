{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Supervised Learning - Classification](#toc1_)    \n",
        "  - [K-Nearest Neighbors (distance-based)](#toc1_1_)    \n",
        "    - [Overfitting check](#toc1_1_1_)    \n",
        "    - [Hyperparameter tuning](#toc1_1_2_)    \n",
        "  - [Logistic Regression (equation-based)](#toc1_2_)    \n",
        "    - [Evaluation metrics](#toc1_2_1_)    \n",
        "  - [Decision Trees (tree-based)](#toc1_3_)    \n",
        "    - [Hyperparameter Tuning](#toc1_3_1_)    \n",
        "    - [Review decision tree](#toc1_3_2_)    \n",
        "    - [Feature importance](#toc1_3_3_)    \n",
        "    - [ðŸ’¥ **Bonus**: Lolliipop charts in Python](#toc1_3_4_)    \n",
        "  - [Support Vector Machines](#toc1_4_)    \n",
        "    - [Cross Validation](#toc1_4_1_)    \n",
        "    - [ðŸ’¥ **Bonus**: Stratification in sklearn](#toc1_4_2_)    \n",
        "- [Resources](#toc2_)    \n",
        "- [Acknowledgements](#toc3_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Supervised Learning - Classification](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwD_ZUaGN9b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "cancer = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW2VFh5HWt9p",
        "outputId": "39d7a41d-5bbd-4728-e3a1-00cc80a7080c"
      },
      "outputs": [],
      "source": [
        "# description of the dataset\n",
        "print(cancer['DESCR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhgjXKLcN70q"
      },
      "outputs": [],
      "source": [
        "# 212 people with cancer\n",
        "# 357 people without cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "_HU2z7Y9OGNE",
        "outputId": "f13169f0-e4b7-45e3-a8a4-16d3b3d6abb3"
      },
      "outputs": [],
      "source": [
        "# Extract dataset into pandas\n",
        "features = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n",
        "labels = pd.Series(cancer['target'], name = 'labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display features & labels\n",
        "display(features.head())\n",
        "display(labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz7qxZLjPngk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[K-Nearest Neighbors (distance-based)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4Tt0KrCOK3X"
      },
      "outputs": [],
      "source": [
        "# Initialize model with N=9\n",
        "model = KNeighborsClassifier(n_neighbors=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ethcz_bO8ID"
      },
      "outputs": [],
      "source": [
        "# Train model & predict\n",
        "model = model.fit(X_train, y_train)\n",
        "model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonm3bgVuk_1",
        "outputId": "dbedc3ba-75eb-44c2-edbf-1cb54201e0a0"
      },
      "outputs": [],
      "source": [
        "# Compare predictions to reality\n",
        "np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR_AmrvIu6xN",
        "outputId": "1eda756a-a2e4-4d07-c29d-651ed9775113"
      },
      "outputs": [],
      "source": [
        "# Compute overall accuracy\n",
        "accuracy_score(model.predict(X_test),np.array(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute classification report\n",
        "print(classification_report(model.predict(X_test), np.array(y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_1_'></a>[Overfitting check](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Has my model simply memorized the data or did it infer some patterns and relationships from the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5902rD43PDu6",
        "outputId": "b46eb7f0-fb72-4031-8bc9-a1ab19b56400"
      },
      "outputs": [],
      "source": [
        "# once the model is trained you can call the score method, to compare results of test predictions with actual values -> returns the accuracy\n",
        "print(\"test data accuracy was \",model.score(X_test,y_test))\n",
        "\n",
        "# you should always also see the accuracy of the training\n",
        "print(\"train data accuracy was \", model.score(X_train, y_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute classification report\n",
        "print(classification_report(model.predict(X_train), np.array(y_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model peforms better on the test data than on the training data (unusual!). This means our model is slightly **underfit**. \n",
        "\n",
        "In this case, we would add either more data points or more features to further increase the accuracy or we would remove/reduce techniques used to prevent overfitting, e.g. regularization (which we'll talk about in the regression class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_1_2_'></a>[Hyperparameter tuning](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One strategy to deal with underfitting/overfitting is to change the parameters of the model. For the KNN algorithm, the main parameter is the **number of neighbors**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "6LlFcKE0RQy7",
        "outputId": "0a117f07-bd93-4e68-cc0f-f771fc431fc7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# hyerparameter tuning - extract train-test scores into lists\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "# try n_neighbors from 1 to 30\n",
        "neighbors_settings = range(1, 30)\n",
        "\n",
        "for n_neighbors in neighbors_settings:\n",
        "  # Build the model\n",
        "  clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "  # Train the model\n",
        "  clf.fit(X_train, y_train)\n",
        "  # record training set accuracy\n",
        "  train_accuracy.append(clf.score(X_train, y_train))\n",
        "  # record generalization accuracy\n",
        "  test_accuracy.append(clf.score(X_test, y_test))\n",
        "\n",
        "# Plot results\n",
        "plt.plot(neighbors_settings, train_accuracy, label=\"train accuracy\")\n",
        "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"n_neighbors\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Which is the optimal number of neighbors?** Let's review using plotly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding a plotly chart for comparison\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=list(neighbors_settings), y=train_accuracy, name='Training Accuracy'))\n",
        "fig.add_trace(go.Scatter(x=list(neighbors_settings), y=test_accuracy, name='Testing Accuracy'))\n",
        "fig.update_layout(xaxis_title='Accuracy', yaxis_title='No neighbors', title='')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10 - 15 neighbours seems to be the optimal point as it's the maximum training score we achieve, in spite of underfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[Logistic Regression (equation-based)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIvR2qI7O4jZ",
        "outputId": "7104ef49-a2de-47d7-e2d6-ada38e4a3ae7"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize & fit the model\n",
        "model = LogisticRegression()\n",
        "model = model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhyjXg8_O4l7",
        "outputId": "abbaeb6e-94af-4c52-acd9-4599e569720a"
      },
      "outputs": [],
      "source": [
        "# Basic Accuracy data\n",
        "print(\"test data accuracy was \",model.score(X_test,y_test))\n",
        "print(\"train data accuracy was \", model.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WAu051M0hFf",
        "outputId": "f162f6d9-6e01-4383-bd2c-57efd85667d6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get overall accuracy\n",
        "pred = model.predict(X_test)\n",
        "accuracy_score(y_test, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_2_1_'></a>[Evaluation metrics](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "sV14lLFlyE48",
        "outputId": "08c188b4-2098-4d2b-e355-431ae8f4a013"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get confusion matrix, convert to dataframe\n",
        "cm = pd.DataFrame(confusion_matrix(y_test, pred))\n",
        "cm.rename({0: 'No - Pred', 1: 'Yes - Pred'}, axis=1, inplace=True)\n",
        "cm.rename({0: 'No - True', 1: 'Yes - True'}, axis=0, inplace=True)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "px.imshow(cm, text_auto=True, color_continuous_scale='RdBu', color_continuous_midpoint=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kIxZPzrQGJS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Extract precision & recall separately\n",
        "print(precision_score(y_test,pred))\n",
        "print(recall_score(y_test,pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptlo3gWyTWQz"
      },
      "outputs": [],
      "source": [
        "# Review full classification report\n",
        "print(classification_report(y_test,pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is 99% accuracy good when predicting cancer outcomes? \n",
        "\n",
        "[Not really - 23 minutes worth of explanation for this one, feat. Bayes theorem](https://www.youtube.com/watch?v=lG4VkPoG3ko). \n",
        "\n",
        "In the video: **PPV ~ precision** and **sensitivity ~ recall**. For more on this matter, check out this article: [Data Science in Medicine â€” Precision & Recall or Specificity & Sensitivity?](https://towardsdatascience.com/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_3_'></a>[Decision Trees (tree-based)](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://imgs.search.brave.com/-MSTVa2-jo6LRnfBlwZ6P2ogZFJuJ431Os0ha2p2GuU/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/djIvMCpsV0R1a2dJ/NE9RNkZ5RHpzLnBu/Zw)  \n",
        "(Source: [An Exhaustive Guide to Decision Tree Classification in Python 3.x, Towards Data Science](https://towardsdatascience.com/an-exhaustive-guide-to-classification-using-decision-trees-8d472e77223f?gi=ec8e06014983))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FChWxd_cS3RX"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "GA0z3UydH0he",
        "outputId": "60c30c11-358c-43e4-9e9b-3c3012c026a3"
      },
      "outputs": [],
      "source": [
        "# Instantiate and fit decision tree\n",
        "model = DecisionTreeClassifier(max_depth=10)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision trees (and their family, Random Forests) are very prone to overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpoYjktqIY8Y",
        "outputId": "cf8b31b6-8ba8-4114-9da5-f78dffc6be40"
      },
      "outputs": [],
      "source": [
        "# Review overall accuracy scores\n",
        "print(\"test data accuracy was \",model.score(X_test,y_test))\n",
        "print(\"train data accuracy was \",model.score(X_train,y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_3_1_'></a>[Hyperparameter Tuning](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repeat the fitting process using different values for the `max_depth` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwPQkrI3I39R"
      },
      "outputs": [],
      "source": [
        "max_depth = range(1, 30)\n",
        "test = []\n",
        "train = []\n",
        "\n",
        "for depth in max_depth:\n",
        "  model = DecisionTreeClassifier(max_depth= depth)\n",
        "  model.fit(X_train, y_train)\n",
        "  test.append(model.score(X_test,y_test))\n",
        "  train.append(model.score(X_train,y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Review the train/test accuracy with different parameter values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "fXkWZFecJnXf",
        "outputId": "3ce758c2-5a16-4c94-ba27-b3c01971cc79"
      },
      "outputs": [],
      "source": [
        "plt.plot(train, label=\"training accuracy\")\n",
        "plt.plot(test, label=\"test accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"n_depth\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Which is the ideal depth?** Let's review in plotly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=list(max_depth), y=train, name='Training Accuracy'))\n",
        "fig.add_trace(go.Scatter(x=list(max_depth), y=test, name='Testing Accuracy'))\n",
        "fig.update_layout(xaxis_title='Accuracy', yaxis_title='Max Tree Depth', title='')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best performing `max_depth` is 24, although 2-4 perform almost equally well. Here, we'd decide on a depth of 3 which is less likely to overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_3_2_'></a>[Review decision tree](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "eF7ip_drL158",
        "outputId": "475e6aa7-2d5f-4fad-ec7f-14a64bdea4b1"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier(max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "from sklearn import tree\n",
        "fig = plt.figure(figsize=(25,20))\n",
        "_ = tree.plot_tree(model, \n",
        "                   feature_names=cancer.feature_names,  \n",
        "                   class_names=[\"malignant\", \"benign\"],\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_3_3_'></a>[Feature importance](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "ejT27khSRl4R",
        "outputId": "d97b0685-f2a9-4e34-8cee-96969cd2a0b0"
      },
      "outputs": [],
      "source": [
        "# Review features\n",
        "X_train.head().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Decision trees automaticaly give you feature importance based on how many times they split on a given feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.feature_names_in_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MbhOTCJMMPO",
        "outputId": "f29b7fe3-4898-4ba0-fba7-4face6ed200d"
      },
      "outputs": [],
      "source": [
        "model.feature_importances_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's not very clear to see which features are the most important by comparing the dataframe to the numpy array, so we will plot the values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "suKAzupUOHDZ",
        "outputId": "1b173ae2-2421-474f-a7f4-3fa5a63d7d59"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importances_cancer(model):\n",
        "  n_features = cancer.data.shape[1]\n",
        "  plt.barh(range(n_features), model.feature_importances_, align='center')\n",
        "  plt.yticks(np.arange(n_features), cancer.feature_names)\n",
        "  plt.xlabel(\"Feature importance\")\n",
        "  plt.ylabel(\"Feature\")\n",
        "plot_feature_importances_cancer(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.histogram(x=model.feature_importances_, y=model.feature_names_in_, category_orders={'category': 'total descending'})\n",
        "fig.update_layout(xaxis_title='Feature importance', height=600)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_3_4_'></a>[ðŸ’¥ **Bonus**: Lolliipop charts in Python](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_importances = pd.DataFrame(model.feature_names_in_, columns=['feature'])\n",
        "feat_importances['importance'] = model.feature_importances_\n",
        "feat_importances = feat_importances.sort_values(by=['importance'],\n",
        "                    ascending=False).iloc[0:15]\n",
        "fig = go.Figure()\n",
        "# Draw points\n",
        "fig.add_trace(go.Scatter(x=feat_importances[\"importance\"], \n",
        "                          y=feat_importances[\"feature\"],\n",
        "                          mode='markers',\n",
        "                          marker_color='darkblue',\n",
        "                          marker_size=10))\n",
        "# Draw lines\n",
        "for i in range(0, len(feat_importances)):\n",
        "               fig.add_shape(type='line',\n",
        "                              x0 = 0, y0 = i,\n",
        "                              x1 = feat_importances[\"importance\"][i],\n",
        "                              y1 = i,\n",
        "                              line=dict(color='crimson', width = 3))\n",
        "# Set title\n",
        "fig.update_layout(title_text = \n",
        "                   \"Top 15 feature importances\",\n",
        "                   title_font_size = 30)\n",
        "# Set x-axes range\n",
        "fig.update_xaxes(title = 'Feature importance' , range=[0, 1])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_4_'></a>[Support Vector Machines](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> SVMs are used when there are two categories and no obvious linear classifier that separates them in a nice way. (OrtusAI + StatQuest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://editor.analyticsvidhya.com/uploads/1403824.png)  \n",
        "(Source: [Guide on Support Vector Machine (SVM) Algorithm, Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of SVC boundaries: \n",
        " \n",
        "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png)\n",
        "(Source: [sklearn documentation](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5pxovgMON0h",
        "outputId": "c8c99891-e312-4675-ae6a-05f5d2d5a8e2"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Initialize and fit model\n",
        "model = LinearSVC()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-2CYFvQ91Jr",
        "outputId": "215fdca0-ac6b-4f38-90d6-4ebd5fd07b6d"
      },
      "outputs": [],
      "source": [
        "# Review overall accuracy score\n",
        "print(model.score(X_train, y_train))\n",
        "print(model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_4_1_'></a>[Cross Validation](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is cross-validation?\n",
        "\n",
        "![](https://imgs.search.brave.com/tEBDW7f_GRHyGUhYVI0mmwKHv5NYPdYEFKxDqBUF3mk/rs:fit:860:0:0/g:ce/aHR0cHM6Ly93d3cu/c2VjdGlvbi5pby9l/bmdpbmVlcmluZy1l/ZHVjYXRpb24vaG93/LXRvLWltcGxlbWVu/dC1rLWZvbGQtY3Jv/c3MtdmFsaWRhdGlv/bi81LWZvbGQtY3Yu/anBlZw)  \n",
        "(Source: [How to Implement K fold Cross-Validation in Scikit-Learn, Section.io](https://www.section.io/engineering-education/how-to-implement-k-fold-cross-validation/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE7NJhLlCHJj",
        "outputId": "dc6eec0d-5522-4e8f-f96f-1cd49c4205f3"
      },
      "outputs": [],
      "source": [
        "# Applying an example of cross validation\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "model = LinearSVC()\n",
        "results = cross_validate(model, cancer['data'], cancer['target'], cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QykhB1OFDGWi",
        "outputId": "bf557c51-4d2a-4eaa-c1ac-5faf39b0f668"
      },
      "outputs": [],
      "source": [
        "# Review test scores per validation set\n",
        "results['test_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlahjjepVbQr",
        "outputId": "92521df8-23dc-40db-821a-704bbc26ac27"
      },
      "outputs": [],
      "source": [
        "# Review overall test score\n",
        "results['test_score'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why cross-validation?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_4_2_'></a>[ðŸ’¥ **Bonus**: Stratification in sklearn](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "salary = pd.read_csv('https://raw.githubusercontent.com/sabinagio/data-analytics/main/data/salaries.csv')\n",
        "salary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "salary.Experience.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When predicting salary we would want to have an equal distribution of experience in both the train and test sets, i.e. we want to stratify our train-test split by experience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split with stratification\n",
        "X = salary.drop('Salary', axis=1)\n",
        "y = salary['Salary']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25, stratify=X['Experience'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review train set proportions\n",
        "X_train.Experience.value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review test set proportions\n",
        "X_test.Experience.value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how our proportions look like without stratification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split without stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review train set proportions\n",
        "X_train.Experience.value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review test set proportions\n",
        "X_test.Experience.value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why stratification?** This is for you to do some research on ðŸ˜‰  \n",
        "\n",
        "*(Hint: It has something to do with sampling... remember that lesson in inferential statistics?)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Resources](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [Decision Trees (StatQuest) - 18 mins](https://www.youtube.com/watch?v=_L39rN6gz7Y)\n",
        "- [Cross-Validation (StatQuest) - 6 mins](https://www.youtube.com/watch?v=fSytzGwwBVw&t=0s)\n",
        "- Support Vector Machine (StatQuest)\n",
        "    - [Main Ideas - 20 mins](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
        "    - [The Polynomial Kernel - 7 mins](https://www.youtube.com/watch?v=Toet3EiSFcM&t=0s)\n",
        "    - [The Radial (RBF) Kernel - 16 mins](https://www.youtube.com/watch?v=Qc5IyLW_hns&t=0s)\n",
        "- [Support Vector Machine - multi-class implementation](https://archive.is/20230328072327/https://towardsdatascience.com/multiclass-classification-with-support-vector-machines-svm-kernel-trick-kernel-functions-f9d5377d6f02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[Acknowledgements](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thank you, David Henriques, for your awesome class structure and content!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "18wn2m4X0kjg",
        "m6y3oMZ20pBj",
        "hS-TCKLo1Jy-",
        "gFpm1iPG1XvV"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
